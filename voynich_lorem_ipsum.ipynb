{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plik `FSG.txt' ju≈º istnieje, bez pobierania.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# http://www.voynich.nu/transcr.html#FSG\n",
    "! wget -nc http://www.ic.unicamp.br/~stolfi/voynich/mirror/reeds/docs/FSG.txt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# clean transcription\n",
    "import re\n",
    "\n",
    "filename = \"FSG.txt\"\n",
    "raw_text = open(filename).read()\n",
    "without_comments_text = re.sub(re.compile(\"(#|\\\\(\\\\|\\\\{l).*?\\n\" ) ,\"\" ,raw_text)\n",
    "without_or = re.sub('\\\\(.*?\\\\)','?', without_comments_text)\n",
    "without_trash = re.sub('(\\\\(|\\\\||\\\\)|_|- | )', '', without_or)\n",
    "clean = re.sub('\\x0c','', without_trash)\n",
    "\n",
    "filename_cleaned = \"FSG_cleaned.txt\"\n",
    "with open(filename_cleaned, \"w\") as text_file:\n",
    "    print(clean, file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input\n",
    "vm_filename = \"FSG_cleaned_small.txt\" # \"FSG_cleaned_small.txt\"\n",
    "vm_text = open(vm_filename).read()\n",
    "\n",
    "# convert characters the to integers\n",
    "chars = sorted(list(set(vm_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  6222\n",
      "Total Vocab:  27\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(vm_text)\n",
    "n_vocab = len(char_to_int)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  6182\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 40\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(vm_text) - seq_length, 1):\n",
    "\tseq_in = vm_text[i:i + seq_length]\n",
    "\tseq_out = vm_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "# simple model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# -1\n",
    "model = Sequential()\n",
    "model.add(LSTM(2048, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(1024, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"simple_model_40_weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, callbacks=callbacks_list, batch_size=128, nb_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# load the network weights\n",
    "filename = \"simple_model_40_weights-improvement-19-1.0051.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = {v: k for k, v in char_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" DTG,DTOK,TOR,TCDCOE,ODA-\n",
      "GHTCCAR,ODCOE,H \"\n",
      "ZOE,8AM,HZOE-\n",
      "GTOR,TOE,HZOE,TOH,TO8G,TO8G,TOE,ODTOE,8AM-\n",
      "SO,TOE,SOE,HZOE,SO8A,TOE,8AM,OHTOE,8AM-\n",
      "SO,TOE,SOE,HZOE,SO8AN,TOE,8AM,HZOE,8AM-\n",
      "SOR,TOE,HZOE,8AM,HZOE,8AM,HZOE,8AM,HZOR,TOHZG-\n",
      "4TOR,TOE,HZOE,TOH,TOE,HZOE,8AM,HZOE,8AM,\n",
      "ODTO,GZG,DTOR,TOE,O8G,8OE,8A,8OE,TO8G-\n",
      "ODOE,TOE,SOE,8AM,ODTOE,8AIIR,TOR,OHA\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(300):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result + \"\")\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
